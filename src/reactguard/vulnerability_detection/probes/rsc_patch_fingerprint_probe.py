# SPDX-FileCopyrightText: 2025 Theori Inc.
# SPDX-License-Identifier: AGPL-3.0-or-later

"""Next.js Server Action discovery and probe helpers."""

from __future__ import annotations

import html
import json
import re
from html.parser import HTMLParser
from typing import Any
from urllib.parse import urlparse

from ...config import load_http_settings
from ...http.crawl import crawl_same_origin_html
from ...http.models import HttpRequest, RetryConfig
from ...http.retry import send_with_retries
from ...http.url import build_endpoint_candidates
from ...http.utils import get_http_client


def _user_agent() -> str:
    return load_http_settings().user_agent


def _scan_once(
    url: str,
    *,
    method: str = "GET",
    headers: dict[str, str] | None = None,
    body: str | bytes | None = None,
    allow_redirects: bool = True,
) -> dict[str, Any]:
    """
    Issue a single HTTP request (no retries) and return a request_with_retries-compatible mapping.

    These probes are intended to be deterministic and bounded.
    """
    settings = load_http_settings()
    request_headers = dict(headers or {})
    request_headers.setdefault("User-Agent", settings.user_agent)
    client = get_http_client()

    request = HttpRequest(
        url=url,
        method=method,
        headers=request_headers,
        body=body,
        timeout=None,
        allow_redirects=allow_redirects,
    )

    response = send_with_retries(client, request, retry_config=RetryConfig(max_attempts=1))

    result: dict[str, Any] = {
        "ok": response.ok,
        "status_code": response.status_code,
        "headers": response.headers,
        "body": response.text,
        "body_snippet": response.body_snippet,
        "url": response.url or url,
        "error_message": response.error_message,
        "error_type": response.error_type,
        "meta": dict(response.meta or {}),
    }
    if result.get("ok") is False and result.get("error_message") and result.get("error") is None:
        result["error"] = result["error_message"]
    return result


_NEXT_ACTION_VALUE_BEFORE_NAME_RE = re.compile(r'value=[\'"]([^\'"]+)[\'"][^>]*name=[\'"]\$ACTION_[^\'"]+[\'"]')
_NEXT_ACTION_NAME_BEFORE_VALUE_RE = re.compile(r'name=[\'"]\$ACTION_[^\'"]+[\'"][^>]*value=[\'"]([^\'"]+)[\'"]')
_NEXT_ACTION_JSON_ID_RE = re.compile(r'"id"\s*:\s*"([^"]+)"')
_NEXT_ACTION_ESCAPED_ID_RE = re.compile(r"&quot;id&quot;:&quot;([^&]+)&quot;")


class _NextActionInputParser(HTMLParser):
    def __init__(self) -> None:
        super().__init__()
        self.values: list[str] = []

    def handle_starttag(self, tag: str, attrs: list[tuple[str, str | None]]) -> None:
        if tag.lower() != "input":
            return
        attr_map = {str(k).lower(): ("" if v is None else str(v)) for k, v in attrs if k}
        name = (attr_map.get("name") or "").strip()
        if not name.startswith("$ACTION_"):
            return
        value = attr_map.get("value")
        if value:
            self.values.append(value)


def _extract_nextjs_action_id_from_html(html_text: str) -> str | None:
    if not html_text:
        return None

    # Fast path: look for the escaped JSON payload in the `$ACTION_*` hidden input value.
    match = _NEXT_ACTION_ESCAPED_ID_RE.search(html_text)
    if match:
        return match.group(1)

    for regex in (_NEXT_ACTION_NAME_BEFORE_VALUE_RE, _NEXT_ACTION_VALUE_BEFORE_NAME_RE):
        match = regex.search(html_text)
        if not match:
            continue

        raw_value = html.unescape(match.group(1)).strip()
        if not raw_value:
            continue

        try:
            parsed = json.loads(raw_value)
            if isinstance(parsed, dict) and parsed.get("id"):
                return str(parsed["id"])
        except Exception:
            fallback = _NEXT_ACTION_JSON_ID_RE.search(raw_value)
            if fallback:
                return fallback.group(1)

    # Fallback: parse <input> tags to support attribute values containing quotes (e.g. single-quoted JSON).
    parser = _NextActionInputParser()
    try:
        parser.feed(html_text)
    except Exception:
        return None

    for value in parser.values:
        raw_value = html.unescape(value).strip()
        if not raw_value:
            continue
        try:
            parsed = json.loads(raw_value)
            if isinstance(parsed, dict) and parsed.get("id"):
                return str(parsed["id"])
        except Exception:
            fallback = _NEXT_ACTION_JSON_ID_RE.search(raw_value)
            if fallback:
                return fallback.group(1)

    return None


_NEXT_ACTION_HEADER_LITERAL_RE = re.compile(r'["\']next-action["\']\s*:\s*["\']([^"\']+)["\']', re.IGNORECASE)
_NEXT_ACTION_HEADER_SET_RE = re.compile(r'\.set\(\s*["\']next-action["\']\s*,\s*["\']([^"\']+)["\']\s*\)', re.IGNORECASE)
_NEXT_ACTION_HEADER_APPEND_RE = re.compile(r'\.append\(\s*["\']next-action["\']\s*,\s*["\']([^"\']+)["\']\s*\)', re.IGNORECASE)
_NEXT_ACTION_HEADER_BRACKET_RE = re.compile(r'\[\s*["\']next-action["\']\s*\]\s*=\s*["\']([^"\']+)["\']', re.IGNORECASE)
_NEXT_ACTION_HEX_TOKEN_RE = re.compile(r"\b[0-9a-f]{42}\b", re.IGNORECASE)
# Next.js dev bundles may embed RSC runtime sources inside string literals (e.g. `case \"h\":`), so
# accept an optional escaping backslash before each quote.
#
# To avoid false positives from unrelated switch statements, require the `case "T":` arm to appear
# shortly after the marker arm (this mirrors parseModelString-style switches in RSC runtimes).
_NEXT_RSC_MARKER_CASE_H_RE = re.compile(r"case\s*\\?[\"']h\\?[\"']\s*:.{0,2000}?case\s*\\?[\"']T\\?[\"']\s*:", re.DOTALL)
_NEXT_RSC_MARKER_CASE_F_RE = re.compile(r"case\s*\\?[\"']F\\?[\"']\s*:.{0,2000}?case\s*\\?[\"']T\\?[\"']\s*:", re.DOTALL)

_SCRIPT_SRC_RE = re.compile(r"<script[^>]+src=[\"']([^\"']+)[\"']", re.IGNORECASE)
_LINK_HREF_RE = re.compile(r"<link[^>]+href=[\"']([^\"']+)[\"']", re.IGNORECASE)
_IMPORT_CALL_RE = re.compile(r'import\(\s*["\']([^"\']+)["\']\s*\)', re.IGNORECASE)


def _same_origin(a: str, b: str) -> bool:
    pa = urlparse(a)
    pb = urlparse(b)
    return (pa.scheme, pa.netloc) == (pb.scheme, pb.netloc)


def _looks_like_js_asset(path: str) -> bool:
    if not path:
        return False
    candidate = str(path).split("?", 1)[0].split("#", 1)[0].lower()
    return candidate.endswith((".js", ".mjs", ".ts", ".tsx", ".jsx"))


def _extract_next_action_header_ids(text: str) -> list[str]:
    if not text:
        return []

    candidates: list[str] = []
    for regex in (
        _NEXT_ACTION_HEADER_LITERAL_RE,
        _NEXT_ACTION_HEADER_SET_RE,
        _NEXT_ACTION_HEADER_APPEND_RE,
        _NEXT_ACTION_HEADER_BRACKET_RE,
    ):
        for match in regex.finditer(text):
            value = (match.group(1) or "").strip()
            if not value:
                continue
            if any(ch.isspace() for ch in value):
                continue
            if len(value) > 512:
                continue
            candidates.append(value)

    # De-dupe while preserving order.
    return list(dict.fromkeys(candidates))


def _extract_next_action_hex_tokens(text: str) -> list[str]:
    if not text:
        return []
    candidates = [m.group(0) for m in _NEXT_ACTION_HEX_TOKEN_RE.finditer(text) if m.group(0)]

    # De-dupe while preserving order (case-insensitive).
    out: list[str] = []
    seen: set[str] = set()
    for token in candidates:
        key = token.lower()
        if key in seen:
            continue
        seen.add(key)
        out.append(token)
    return out


def discover_next_action_ids(
    url: str,
    *,
    max_pages: int | None = None,
    max_depth: int | None = None,
    max_assets: int = 20,
) -> list[str]:
    """
    Best-effort discovery of valid `Next-Action` IDs from same-origin content.

    Sources (GET-only, bounded, landing-page only):
    - Next.js `$ACTION_*` hidden input payloads in HTML
    - Literal `Next-Action` header values embedded in inline scripts / JS bundles
    """
    if not url:
        return []

    action_ids: list[str] = []
    html_action_ids: list[str] = []
    asset_urls: list[str] = []

    effective_max_pages = 6 if max_pages is None else max_pages
    effective_max_depth = 2 if max_depth is None else max_depth
    pages = crawl_same_origin_html(
        url,
        max_pages=effective_max_pages,
        max_depth=effective_max_depth,
    )
    origin_url = pages[0].url if pages else url
    for page in pages:
        body = page.body or ""
        if not body:
            continue

        action_ids.extend(_extract_next_action_header_ids(body))
        action_ids.extend(_extract_next_action_hex_tokens(body))

        next_id = _extract_nextjs_action_id_from_html(body)
        if next_id:
            html_action_ids.append(next_id)

        for regex in (_SCRIPT_SRC_RE, _LINK_HREF_RE, _IMPORT_CALL_RE):
            for match in regex.finditer(body):
                raw = (match.group(1) or "").strip()
                if not raw:
                    continue
                if raw.startswith(("data:", "javascript:", "mailto:", "tel:")):
                    continue
                if raw.startswith(("http://", "https://")):
                    # Only follow same-origin absolute URLs.
                    if not _same_origin(origin_url, raw):
                        continue
                    candidate_urls = [raw]
                else:
                    # Support both root-relative and base-relative paths.
                    candidate_urls = build_endpoint_candidates(page.url or url, raw)
                for candidate_url in candidate_urls:
                    if not candidate_url or not _same_origin(origin_url, candidate_url):
                        continue
                    asset_urls.append(candidate_url)

    asset_urls = list(dict.fromkeys([u for u in asset_urls if u]))
    js_assets = [u for u in asset_urls if _looks_like_js_asset(u)]

    for asset_url in js_assets[:max_assets]:
        resp = _scan_once(
            asset_url,
            method="GET",
            headers={"Accept": "application/javascript, text/javascript, */*"},
            allow_redirects=True,
        )
        if not resp.get("ok"):
            continue
        body = str(resp.get("body") or resp.get("body_snippet") or "")
        if not body:
            continue
        action_ids.extend(_extract_next_action_header_ids(body))
        action_ids.extend(_extract_next_action_hex_tokens(body))

    # De-dupe while preserving order.
    merged = [*action_ids, *html_action_ids]
    return list(dict.fromkeys([a for a in merged if a]))


def discover_nextjs_server_reference_marker(
    url: str,
    *,
    max_pages: int | None = None,
    max_depth: int | None = None,
    max_assets: int = 20,
) -> str | None:
    """
    Best-effort discovery of the React Server Components server-reference marker used by Next.js bundles.

    This is a bounded, GET-only heuristic that scans same-origin JS assets for a parseModelString-style
    switch case:
    - `case "F":` in pre-PR#35345 React builds
    - `case "h":` in post-PR#35345 React builds

    Returns: "F", "h", or None (ambiguous / not found).
    """
    if not url:
        return None

    effective_max_pages = 6 if max_pages is None else max_pages
    effective_max_depth = 2 if max_depth is None else max_depth

    pages = crawl_same_origin_html(
        url,
        max_pages=effective_max_pages,
        max_depth=effective_max_depth,
    )
    origin_url = pages[0].url if pages else url

    asset_urls: list[str] = []
    for page in pages:
        body = page.body or ""
        if not body:
            continue
        for regex in (_SCRIPT_SRC_RE, _LINK_HREF_RE, _IMPORT_CALL_RE):
            for match in regex.finditer(body):
                raw = (match.group(1) or "").strip()
                if not raw:
                    continue
                if raw.startswith(("data:", "javascript:", "mailto:", "tel:")):
                    continue
                if raw.startswith(("http://", "https://")):
                    if not _same_origin(origin_url, raw):
                        continue
                    candidate_urls = [raw]
                else:
                    candidate_urls = build_endpoint_candidates(page.url or url, raw)
                for candidate_url in candidate_urls:
                    if not candidate_url or not _same_origin(origin_url, candidate_url):
                        continue
                    asset_urls.append(candidate_url)

    asset_urls = list(dict.fromkeys([u for u in asset_urls if u]))
    js_assets = [u for u in asset_urls if _looks_like_js_asset(u)]

    saw_h = False
    saw_f = False
    for asset_url in js_assets[:max_assets]:
        resp = _scan_once(
            asset_url,
            method="GET",
            headers={"Accept": "application/javascript, text/javascript, */*"},
            allow_redirects=True,
        )
        if not resp.get("ok"):
            continue
        body = str(resp.get("body") or resp.get("body_snippet") or "")
        if not body:
            continue
        if _NEXT_RSC_MARKER_CASE_H_RE.search(body):
            saw_h = True
        if _NEXT_RSC_MARKER_CASE_F_RE.search(body):
            saw_f = True
        if saw_h and saw_f:
            break

    if saw_h and not saw_f:
        return "h"
    if saw_f and not saw_h:
        return "F"
    return None


def discover_nextjs_action_id(
    url: str,
) -> str | None:
    """
    Best-effort extraction of a valid Next.js Server Action ID from the rendered HTML.

    Next.js Server Actions require a valid `Next-Action` header. Some targets embed an action ID in
    server-rendered form payloads (hidden `$ACTION_*` inputs), which we can scrape via a bounded GET.
    """
    result = _scan_once(
        url,
        method="GET",
        headers={"Accept": "text/html, */*"},
    )
    body = str(result.get("body") or result.get("body_snippet") or "")
    return _extract_nextjs_action_id_from_html(body)


def discover_nextjs_action_entrypoint(
    url: str,
    *,
    max_pages: int | None = None,
    max_depth: int | None = None,
) -> tuple[str, str] | None:
    """
    Discover a (page_url, action_id) pair for Next.js Server Actions.

    Attempts to find a (page_url, action_id) pair via a bounded, same-origin, GET-only crawl.

    Note: This uses a bounded, same-origin, GET-only crawl to discover action forms that may live on
    a non-root route (e.g., `/dashboard`). If action forms are not reachable within the crawl budget,
    this may return None.
    """
    if not url:
        return None

    effective_max_pages = 6 if max_pages is None else max_pages
    effective_max_depth = 2 if max_depth is None else max_depth
    pages = crawl_same_origin_html(url, max_pages=effective_max_pages, max_depth=effective_max_depth, follow_links=True)

    origin_url = pages[0].url if pages else url
    asset_referrers: dict[str, str] = {}

    for page in pages:
        body = page.body or ""
        if not body:
            continue

        # Prefer bundle/inline script discovery over `$ACTION_*` hidden inputs.
        inline_candidates = [*_extract_next_action_header_ids(body), *_extract_next_action_hex_tokens(body)]
        if inline_candidates:
            return page.url, inline_candidates[0]

        action_id = _extract_nextjs_action_id_from_html(body)
        if action_id:
            return page.url, action_id

        for regex in (_SCRIPT_SRC_RE, _LINK_HREF_RE, _IMPORT_CALL_RE):
            for match in regex.finditer(body):
                raw = (match.group(1) or "").strip()
                if not raw:
                    continue
                if raw.startswith(("data:", "javascript:", "mailto:", "tel:")):
                    continue
                if raw.startswith(("http://", "https://")):
                    if not _same_origin(origin_url, raw):
                        continue
                    candidate_urls = [raw]
                else:
                    candidate_urls = build_endpoint_candidates(page.url or url, raw)
                for candidate_url in candidate_urls:
                    if not candidate_url or not _same_origin(origin_url, candidate_url):
                        continue
                    asset_referrers.setdefault(candidate_url, page.url or url)

    js_assets = [u for u in asset_referrers if _looks_like_js_asset(u)]
    for asset_url in js_assets[:20]:
        resp = _scan_once(
            asset_url,
            method="GET",
            headers={"Accept": "application/javascript, text/javascript, */*"},
            allow_redirects=True,
        )
        if not resp.get("ok"):
            continue
        body = str(resp.get("body") or resp.get("body_snippet") or "")
        if not body:
            continue
        candidates = [*_extract_next_action_header_ids(body), *_extract_next_action_hex_tokens(body)]
        if candidates:
            return asset_referrers.get(asset_url) or origin_url, candidates[0]

    return None


__all__ = [
    "discover_nextjs_action_id",
    "discover_next_action_ids",
    "discover_nextjs_action_entrypoint",
    "discover_nextjs_server_reference_marker",
]
